"""Core utilities"""

import logging
import inspect
from functools import singledispatch
from copy import deepcopy
from typing import (
    Any, Callable, Iterable, List, Mapping, Optional, Union, Tuple
)

import numpy
from numpy import array as Array

import pandas
from pandas import Categorical, DataFrame, Series
from pipda.symbolic import Reference

from varname import argname

from .exceptions import (
    ColumnNotExistingError, DataUnrecyclable, NameNonUniqueError
)
from .types import is_iterable, is_scalar, Dtype, is_categorical, is_null
from .defaults import DEFAULT_COLUMN_PREFIX, NA_REPR

# logger
logger = logging.getLogger('datar') # pylint: disable=invalid-name
logger.setLevel(logging.INFO)
stream_handler = logging.StreamHandler() # pylint: disable=invalid-name
stream_handler.setFormatter(logging.Formatter(
    '[%(asctime)s][%(name)s][%(levelname)7s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
))
logger.addHandler(stream_handler)

def vars_select(
        all_columns: Iterable[str],
        *columns: Any,
        raise_nonexists: bool = True,
        base0: Optional[bool] = None
) -> List[int]:
    # TODO: support selecting data-frame columns
    """Select columns

    Args:
        all_columns: The column pool to select
        *columns: arguments to select from the pool
        raise_nonexist: Whether raise exception when column not exists
            in the pool
        base0: Whether indexes are 0-based if columns are selected by indexes.
            If not given, will use `datar.base.getOption('index.base.0')`

    Returns:
        The selected indexes for columns

    Raises:
        ColumnNotExistingError: When the column does not exist in the pool
            and raise_nonexists is True.
    """
    from .collections import Collection
    from ..base import unique

    columns = [
        column.name if isinstance(column, Series) else column
        for column in columns
    ]
    selected = Collection(*columns, pool=list(all_columns), base0=base0)
    if raise_nonexists and selected.unmatched and selected.unmatched != {None}:
        raise ColumnNotExistingError(
            f"Columns `{selected.unmatched}` do not exist."
        )
    return unique(selected).astype(int)

def series_expandable(
        df_or_series: Union[DataFrame, Series],
        series_or_df: Union[DataFrame, Series]
) -> bool:
    """Check if a series is expandable"""
    if (not isinstance(df_or_series, (Series, DataFrame)) or
            not isinstance(series_or_df, (Series, DataFrame))):
        return False

    if type(df_or_series) is type(series_or_df):
        if df_or_series.shape[0] < series_or_df.shape[0]:
            series, df = df_or_series, series_or_df
        else:
            df, series = df_or_series, series_or_df
    elif isinstance(df_or_series, Series):
        series, df = df_or_series, series_or_df
    else:
        df, series = df_or_series, series_or_df

    return series.index.name in df.columns

def series_expand(series: Union[DataFrame, Series], df: DataFrame):
    """Expand the series to the scale of a dataframe"""
    if isinstance(series, DataFrame):
        #assert series.shape[1] == 1
        series = series.iloc[:, 0]
    return df[series.index.name].map(series)

def recycle_value(
        value: Any,
        size: int,
        name: Optional[str] = None
) -> Union[DataFrame, numpy.ndarray]:
    """Recycle a value based on a dataframe

    Args:
        value: The value to be recycled
        size: The size to recycle to
        name: The name to show in the error if failed to recycle

    Returns:
        The recycled value
    """
    from ..base import NA

    if is_scalar(value):
        value = [value]

    length = len(value)

    if length not in (0, 1, size):
        name = 'value' if not name else f'`{name}`'
        expect = '1' if size == 1 else f'(1, {size})'
        raise DataUnrecyclable(
            f"Cannot recycle {name} to size {size}, "
            f"expect {expect}, got {length}."
        )

    if isinstance(value, DataFrame):
        if length == size == 0:
            return DataFrame(columns=value.columns)
        if length == 0:
            value = DataFrame([[NA] * value.shape[1]], columns=value.columns)
        if length == 1 and size > length:
            return value.iloc[[0] * size, :].reset_index(drop=True)
        return value

    cats = categorized(value).categories if is_categorical(value) else None

    if length == size == 0:
        return [] if cats is None else Categorical([], categories=cats)

    if length == 0:
        value = [NA]

    if isinstance(value, Series):
        # try to keep Series class
        # some operators can only do with it or with it correctly
        # For example:
        # Series([True, True]) & Series([False, NA]) -> [False, Fa.se]
        # But with numpy.array, it raises error, since NA is a float
        if length == 1 and size > length:
            value = value.iloc[[0] * size].reset_index(drop=True)
        return value

    if isinstance(value, tuple):
        value = list(value)

    # dtype = getattr(value, 'dtype', None)
    if length == 1 and size > length:
        value = list(value) * size

    if cats is not None:
        return Categorical(value, categories=cats)

    is_elem_iter = any(is_iterable(val) for val in value)
    if is_elem_iter:
        # without dtype: VisibleDeprecationWarning
        # return Array(value, dtype=object)
        # The above does not keep [DataFrame()] structure
        return value

    # Avoid numpy.nan to be converted into 'nan' when other elements are string
    out = Array(value)
    if numpy.issubdtype(out.dtype, numpy.str_) and is_null(value).any():
        return Array(value, dtype=object)
    return out


def recycle_df(
        df: DataFrame,
        value: Any,
        df_name: Optional[str] = None,
        value_name: Optional[str] = None
) -> Tuple[DataFrame, Any]:
    """Recycle the dataframe based on value"""
    if length_of(df) == 1:
        df = recycle_value(df, length_of(value), df_name)
    value = recycle_value(value, length_of(df), value_name)
    return df, value

def categorized(data: Any) -> Any:
    """Get the Categorical object"""
    if not is_categorical(data):
        return data
    if isinstance(data, Series):
        return data.values
    return data

@singledispatch
def to_df(data: Any, name: Optional[str] = None) -> DataFrame:
    """Convert an object to a data frame"""
    if is_scalar(data):
        data = [data]

    if name is None:
        return DataFrame(data)

    return DataFrame({name: data})

@to_df.register(numpy.ndarray)
def _(data: numpy.ndarray, name: Optional[str] = None) -> DataFrame:
    if len(data.shape) == 1:
        return DataFrame(data, columns=[name]) if name else DataFrame(data)

    ncols = data.shape[1]
    if isinstance(name, Iterable) and len(name) == ncols:
        return DataFrame(data, columns=name)
    if len(name) == 1 and name and isinstance(name, str):
        return DataFrame(data, columns=[name])
    # ignore the name
    return DataFrame(data)

@to_df.register(DataFrame)
def _(data: DataFrame, name: Optional[str] = None) -> DataFrame:
    if name is None:
        return data
    return DataFrame({f"{name}${col}": data[col] for col in data.columns})

@to_df.register(Series)
def _(data: Series, name: Optional[str] = None) -> DataFrame:
    name = name or data.name
    return data.to_frame(name=name)

# @to_df.register(SeriesGroupBy)
# def _(data: SeriesGroupBy, name: Optional[str] = None) -> DataFrame:
#     name = name or data.obj.name
#     return data.obj.to_frame(name=name).groupby(data.grouper, dropna=False)

def check_column_uniqueness(df: DataFrame, msg: Optional[str] = None) -> None:
    """Check if column names are unique of a dataframe"""
    uniq = set()
    for col in df.columns:
        if col not in uniq:
            uniq.add(col)
        else:
            msg = msg or 'Name is not unique'
            raise NameNonUniqueError(f"{msg}: {col}")

def dict_insert_at(
        container: Mapping[str, Any],
        poskeys: Iterable[str],
        value: Mapping[str, Any],
        remove: bool = False
) -> Mapping[str, Any]:
    """Insert value to a certain position of a dict"""
    ret_items = []
    ret_items_append = ret_items.append
    matched = False
    for key, val in container.items():
        if key == poskeys[0]:
            matched = True
            ret_items.extend(value.items())
            if not remove:
                ret_items_append((key, val))
        elif matched and key in poskeys:
            if not remove:
                ret_items_append((key, val))
        elif matched and key not in poskeys:
            matched = False
            ret_items_append((key, val))
        else:
            ret_items_append((key, val))

    return dict(ret_items)

def name_mutatable_args(
        *args: Union[Series, DataFrame, Mapping[str, Any]],
        **kwargs: Any
) -> Mapping[str, Any]:
    """Convert all mutatable arguments to named mappings, which can be easier
    to mutate later on.

    If there are Expression objects, keep it. So if an objects have multiple
    names and it's built by an Expression, then the name might get lost here.

    Examples:

        >>> s = Series([1], name='a')
        >>> name_mutatable_args(s, b=2)
        >>> # {'a': s, 'b': 2}
        >>> df = DataFrame({'x': [3], 'y': [4]})
        >>> name_mutatable_args(df)
        >>> # {'x': [3], 'y': [4]}
        >>> name_mutatable_args(d=df)
        >>> # {'d$x': [3], 'd$y': [4]}
    """
    ret = {} # order kept

    for i, arg in enumerate(args):
        if isinstance(arg, Series):
            ret[arg.name] = arg
        elif isinstance(arg, dict):
            ret.update(arg)
        elif isinstance(arg, DataFrame):
            ret.update(arg.to_dict('series'))
        elif isinstance(arg, Reference):
            ret[arg.ref] = arg
        else:
            ret[f"{DEFAULT_COLUMN_PREFIX}{i}"] = arg

    for key, val in kwargs.items():
        if isinstance(val, DataFrame):
            val = val.to_dict('series')

        if isinstance(val, dict):
            existing_keys = [
                ret_key for ret_key in ret
                if ret_key == key or ret_key.startswith(f"{key}$")
            ]
            if existing_keys:
                ret = dict_insert_at(ret, existing_keys, val, remove=True)
            else:
                for dkey, dval in val.items():
                    ret[f"{key}${dkey}"] = dval
        else:
            ret[key] = val
    return ret

def arg_match(arg: Any, values: Iterable[Any], errmsg=Optional[str]) -> Any:
    """Make sure arg is in one of the values.

    Mimics `rlang::arg_match`.
    """
    if not errmsg:
        values = list(values)
        name = argname(arg, pos_only=True)
        errmsg = f'`{name}` must be one of {values}.'
    if arg not in values:
        raise ValueError(errmsg)
    return arg

def copy_attrs(
        df1: DataFrame,
        df2: DataFrame,
        deep: bool = True
) -> None:
    """Copy attrs from df2 to df1"""
    for key, val in df2.attrs.items():
        if key.startswith('_'):
            continue
        df1.attrs[key] = deepcopy(val) if deep else val

def nargs(fun: Callable) -> int:
    """Get the number of arguments of a function"""
    return len(inspect.signature(fun).parameters)

def position_at(
        pos: int,
        length: int,
        base0: Optional[bool] = None,
        raise_exc: bool = True
) -> int:
    """Get the 0-based position right at the given pos

    When `pos` is negative, it acts like 0-based, meaning `-1` will anyway
    represent the last position regardless of `base0`

    Args:
        pos: The given position
        length: The length of the pool
        base0: Whether the given `pos` is 0-based
        raise_exc: Raise error if `pos` is out of range?

    Returns:
        The 0-based position
    """
    from .collections import Collection
    coll = Collection(pos, pool=length, base0=base0)
    if raise_exc and coll.error:
        # pylint: disable=raising-bad-type
        raise coll.error
    return coll[0]

def position_after(pos: int, length: int, base0: Optional[bool] = None) -> int:
    """Get the 0-based position right at the given pos

    Args:
        pos: The given position
        length: The length of the pool

    Returns:
        The position before the given position
    """
    base0 = get_option('index.base.0', base0)
    # after 0 with 1-based, should insert to first column
    if not base0 and pos == 0:
        return 0

    return position_at(pos, length, base0) + 1

def get_option(key: str, value: Any = None) -> Any:
    """Get the option with key.

    This is for interal use mostly.

    This is a shortcut for:
    >>> if value is not None:
    >>>     return value
    >>> from datar.base import getOption
    >>> return getOption(key)
    """
    if value is not None:
        return value
    from ..base import getOption
    return getOption(key)

def apply_dtypes(
        df: DataFrame,
        dtypes: Optional[Union[bool, Dtype, Mapping[str, Dtype]]]
) -> None:
    """Apply dtypes to data frame"""
    if dtypes is None or dtypes is False:
        return

    if dtypes is True:
        inferred = df.convert_dtypes()
        for col in df:
            df[col] = inferred[col]
        return

    if not isinstance(dtypes, dict):
        dtypes = dict(zip(df.columns, [dtypes]*df.shape[1]))

    for column, dtype in dtypes.items():
        if column in df:
            df[column] = df[column].astype(dtype)
        else:
            for col in df:
                if col.startswith(f"{column}$"):
                    df[col] = df[col].astype(dtype)

def keep_column_order(df: DataFrame, order: Iterable[str]):
    """Keep the order of columns as given `order`

    We cannot do `df[order]` directly, since `df` may have nested df columns.
    """
    out_columns = []
    for col in order:
        if col in df:
            out_columns.append(col)
        else:
            out_columns.extend(
                (dfcol for dfcol in df.columns if dfcol.startswith(f"{col}$"))
            )
    if set(out_columns) != set(df.columns):
        raise ValueError("Given `order` does not select all columns.")

    return df[out_columns]

def reconstruct_tibble(
        input: DataFrame, # pylint: disable=redefined-builtin
        output: DataFrame,
        ungrouped_vars: Optional[List[str]] = None,
        keep_rowwise: bool = False
) -> DataFrame:
    """Reconstruct the output dataframe based on input dataframe

    Args:
        input: The input data frame
        output: The output data frame
        ungrouped_vars: Variables to exclude from grouping
        keep_rowwise: Whether rowwise structure should be kept

    Return:
        The reconstructed dataframe.
    """
    from ..base import setdiff, intersect
    from ..dplyr import group_vars, group_by_drop_default
    from .grouped import DataFrameGroupBy, DataFrameRowwise

    if ungrouped_vars is None:
        ungrouped_vars = []
    old_groups = group_vars(input)
    new_groups = intersect(setdiff(old_groups, ungrouped_vars), output.columns)

    if isinstance(input, DataFrameRowwise):
        out = DataFrameRowwise(
            output,
            _group_vars=new_groups,
            _group_drop=group_by_drop_default(input)
        ) if keep_rowwise else output
    elif isinstance(input, DataFrameGroupBy) and len(new_groups) > 0:
        out = DataFrameGroupBy(
            output,
            _group_vars=new_groups,
            _group_drop=group_by_drop_default(input)
        )
    else:
        out = output

    copy_attrs(out, input)
    return out

def df_getitem(df: DataFrame, ref: Any) -> Union[DataFrame, Array]:
    """Select columns from a data frame

    If the column is a data frame, select that data frame.
    """
    try:
        return df[ref]
    except KeyError:
        cols = [col for col in df.columns if col.startswith(f'{ref}$')]
        if not cols:
            raise KeyError(ref)
        ret = df.loc[:, cols]
        ret.columns = [col[len(ref)+1:] for col in cols]
        return ret

def df_setitem(
        df: DataFrame,
        name: str,
        value: Any,
        allow_dups: bool = False
) -> DataFrame:
    """Assign an item to a dataframe

    Args:
        df: The data frame
        name: The name of the item
        value: The value to insert
        allow_dups: Allow duplicated names

    Returns:
        df itself or a merged df
    """
    value = recycle_value(value, df.shape[0])
    if isinstance(value, DataFrame):
        # nested df
        value.columns = [f'{name}${col}' for col in value.columns]

        if allow_dups:
            return pandas.concat([df, value], axis=1)

        for col in value.columns:
            df[col] = value[col]
        return df

    if isinstance(value, numpy.ndarray) and value.ndim > 1:
        # keep the list structure
        # otherwise, keep array to have dtype kept
        value = value.tolist()

    # drop the index/match the index of df
    if isinstance(value, Series):
        value = value.values

    if isinstance(value, tuple):
        # ('A', array([1,2,3]))
        # VisibleDeprecationWarning
        value = list(value)

    if not allow_dups:
        df[name] = value

    else:
        df.insert(
            df.shape[1],
            name,
            value,
            allow_duplicates=True
        )

    return df

def fillna_safe(data: Iterable, rep: Any = NA_REPR) -> Iterable:
    """Safely replace NA in data, as we can't just fillna for
    a Categorical data directly.

    Args:
        data: The data to fill NAs
        rep: The replacement for NAs

    Returns:
        Data itself if it has no NAs.
        For Categorical data, `rep` will be added to categories
        Otherwise a Array object with NAs replaced with `rep` is returned with
        dtype object.

    Raises:
        ValueError: when `rep` exists in data
    """
    if rep in data:
        raise ValueError("The value to replace NAs is already present in data.")

    if not is_null(data).any():
        return data

    if is_categorical(data):
        data = categorized(data)
        data = data.add_categories(rep)
        return data.fillna(rep)

    # rep may not be the same dtype as data
    return Series(data).fillna(rep).values

def na_if_safe(
        data: Iterable,
        value: str = NA_REPR,
        dtype: Optional[Dtype] = None
) -> Iterable:
    """Replace value with NA

    Args:
        data: The data to replace value with NA
        value: The value to match
        dtype: Convert the data to dtype after replacement
            If data is Categorical, this is ignored

    Return:
        Array or categorical
    """
    if is_categorical(data):
        # ignore dtype
        data = categorized(data)
        # value to NA, value in categories removed
        return data.replace(value, None)

    from ..base import NA

    out = Series(data).replace(value, NA)
    return out if dtype is None else out.astype(dtype)

def length_of(x: Any) -> int:
    """Get the length of a value, scalar gets 1"""
    if is_scalar(x):
        return 1

    return len(x)
